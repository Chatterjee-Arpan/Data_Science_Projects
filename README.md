# Data_Science_Projects
Collection of Jupyter Notebooks involving Data Science &amp; Machine Learning Projects

* The `Sales Analysis` project analyzes Rossmann store sales by loading and merging daily transaction records with store metadata, performing exploratory data analysis (including missing-data heatmaps, distribution plots, correlation matrices, and outlier checks), enriching the dataset with holiday and promotion information, and finally using **Facebook Prophet** to build and visualize time-series forecasts of future sales at the individual store level.

* The `Sentiment Analysis` project performs sentiment analysis on Amazon Alexa customer reviews by loading a TSV dataset, inspecting and engineering basic features (e.g. review length), conducting exploratory data analysis (including distributions of ratings and sentiment, and word clouds of overall and negative reviews), building a text‑cleaning pipeline (to remove punctuation and stopwords), vectorizing the cleaned text with `CountVectorizer`, and training and evaluating two classifiers: **Multinomial Naive Bayes** and **Logistic Regression**, to predict whether a review is positive or negative.

* The `Credit Risk Analysis` project analyzes German credit-risk data by loading and auditing the dataset (shape, data types, descriptive stats), checking duplicates and missing values (then removing missing rows and dropping the extra index column), and performing EDA with distribution plots for numeric features (`Age`, `Credit amount`, `Duration`), countplots for categorical drivers (`Sex`, `Job`, `Housing`, `Saving accounts`, `Checking account`, `Purpose`), correlation heatmaps, and bivariate visuals (scatter/violin/boxplots) to understand how these features relate to the target `Risk`; it then builds a modeling dataset using selected features, label-encodes categorical variables (and saves the encoders), creates a stratified train/test split, trains and tunes multiple classifiers `(Decision Tree, Random Forest, Extra Trees, and XGBoost)` with **5-fold `GridSearchCV`** while handling **class imbalance (`class_weight='balanced'` / `scale_pos_weight`)**, evaluates accuracy on the hold-out test set, and finally saves the best-performing XGBoost model for reuse. The final product is a **Streamlit** app, which uses the XGBoost model to make predictions.

* The `Fraud Detection` project detects fraudulent transactions by loading and auditing a transaction dataset of ~6 million records (schema checks, missing-value counts, and class distribution for `isFraud` and `isFlaggedFraud`), quantifying overall fraud percentage, and performing EDA with transaction-type distributions and fraud rates by type (with focused analysis on `TRANSFER` and `CASH_OUT`), amount analysis using log-transformed histograms and fraud vs non-fraud boxplots, and behavior-based checks such as engineered balance-difference features (`balanceDiffOriginal`, `balanceDiffDestination`), anomaly counts (negative balance changes), and “zero-after-transfer” patterns where `newbalanceOrig` becomes 0 after certain transfers; it also visualizes fraud occurrence over time using `step` (then drops `step`), inspects the most frequent senders/receivers and top fraud-associated origin accounts, and summarizes numeric relationships via a correlation heatmap; finally, it prepares a modeling dataset by dropping high-cardinality identifiers (`nameOrig`, `nameDest`) and `isFlaggedFraud`, builds a full preprocessing + modeling `Pipeline` (StandardScaler for numeric features and OneHotEncoder for categorical `type`), trains a class-imbalance-aware `LogisticRegression` (`class_weight='balanced'`) with a stratified train/test split, evaluates performance using a classification report, confusion matrix, and test accuracy, and saves the trained pipeline as `fraud_detection_model.pkl` for reuse.

- The `Yelp Business Review Analytics` project builds an end-to-end semi-structured analytics pipeline by splitting a ~5GB Yelp reviews JSONL file (~7M records) into 25 upload-friendly shards in Python (line-count based chunking), landing the review shards plus the Yelp business JSON into an S3 bucket, and bulk-loading both into using `COPY INTO` into raw `VARIANT` staging tables. It then flattens and types the JSON into relational tables (`tbl_yelp_reviews`, `tbl_yelp_businesses`) and enriches each review with sentiment via a Snowflake **Python UDF** (`analyze_sentiment`, Python 3.9 + `textblob`) that maps polarity to Positive, Neutral, or Negative. Finally, it runs SQL analytics using **CTEs**, **LATERAL SPLIT_TO_TABLE** (category explosion), **window functions** with `ROW_NUMBER` + `QUALIFY`, and joins to compute category distribution, most-reviewed categories, top reviewers for restaurant businesses, 3 most recent reviews per business, review seasonality by month, percent of 5-star reviews per business, top 5 most-reviewed businesses per city, rating stability for businesses with 100+ reviews, and top businesses by positive sentiment volume.

- Built an end-to-end `Telco Customer Churn Prediction` project to identify customers most likely to cancel service. I first performed EDA to understand churn behavior and key drivers, noting the dataset’s imbalance (roughly **73:27 non-churn vs churn**) and strong churn patterns for **month-to-month contracts**, **electronic check** payments, **fiber-optic internet**, **low tenure/first year customers**, and customers with **no online security / no tech support**, while **long-term contracts** and **5+ years tenure** showed lower churn. For preprocessing, I converted **TotalCharges** to numeric (dropping a small set of resulting missing rows), engineered a **tenure_group** feature (12-month bins), and one-hot encoded categorical variables. In modeling, I compared **Decision Tree** vs **Random Forest**, handled imbalance using **SMOTEENN**, evaluated using **precision/recall/F1** and **confusion matrix** (instead of accuracy), tested PCA (no improvement), and finalized/pickled the **Random Forest + SMOTEENN** model for deployment in a Flask web app. 
